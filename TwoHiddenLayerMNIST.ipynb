{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TwoHiddenLayerMNIST.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "rX8mhOLljYeM"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jlmcc94303/A-practice-article/blob/master/TwoHiddenLayerMNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "do3t2gfaAeLV"
      },
      "source": [
        "# Tensorflow Neural Network Tutorial\n",
        "for use with *Deep Learning*, by J. L. McClelland & M. Botvinick.\n",
        "\n",
        "To appear in M. Kahana & A. Wagner (Eds), *Oxford Handbook of Human Memory*, 2020, OUP.\n",
        "\n",
        "Based on 'Tensorflow quickstart for beginners' \n",
        "by The Tensorflow Authors. Adapted by J. McClelland."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rX8mhOLljYeM"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Authors,\n",
        "Adaptation Copyright 2020 James L, McClelland"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaMYnlvWc3Od",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DUNzJc4jTj6G"
      },
      "source": [
        "For the original TF quickstart for beginners, follow these links:\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/quickstart/beginner\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/quickstart/beginner.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/quickstart/beginner.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/quickstart/beginner.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "04QgGZc9bF5D"
      },
      "source": [
        "This short introduction uses [Keras](https://www.tensorflow.org/guide/keras/overview) to:\n",
        "\n",
        "1. Build a neural network that classifies images.\n",
        "2. Train this neural network.\n",
        "3. Evaluate the accuracy of the model.\n",
        "4. Examine the output confusability matrix and\n",
        "hidden-layer representational similarity matrix\n",
        "after training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hiH7AC-NTniF"
      },
      "source": [
        "This is a [Google Colaboratory](https://colab.research.google.com/notebooks/welcome.ipynb) notebook file. Python programs are run directly in the browserâ€”a great way to learn and use TensorFlow. To follow this tutorial, run the notebook in Google Colab by clicking the button at the top of this page.\n",
        "\n",
        "1. In Colab, connect to a Python runtime: At the top-right of the menu bar, select *CONNECT*.\n",
        "2. Run all the notebook code cells: Select *Runtime* > *Run all*.\n",
        "\n",
        "Note: To rerun after making changes, it is best to select *Runtime* > *Restart runtime* before selecting *Runtime* > *Run all*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nnrWf3PCEzXL"
      },
      "source": [
        "Download and install TensorFlow 2. Import TensorFlow into your program:\n",
        "\n",
        "Note: It may be necessary to upgrade `pip` to install the TensorFlow 2 package. See the [install guide](https://www.tensorflow.org/install) for details."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0trJmd6DjqBZ",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np; import numpy.matlib; import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import math; import scipy as sp; import scipy.linalg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7NAbSZiaoJ4z"
      },
      "source": [
        "Load and prepare the [MNIST dataset](http://yann.lecun.com/exdb/mnist/). Convert the samples from integers to floating-point numbers:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7FP5258xjs-v",
        "colab": {}
      },
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BPZ68wASog_I"
      },
      "source": [
        "Build the `tf.keras.Sequential` network by stacking layers. We have broken the network into parts\n",
        "to make it easy to access the patterns of activation in the hidden layers of the network.\n",
        "\n",
        "The original tutorial includes Dropout, which you can read about [on this blog](https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/).  To see what happens, uncomment the line inserting Dropout in the code block below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "h3IKyzTCDNGo",
        "colab": {}
      },
      "source": [
        "hidden1 = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "  tf.keras.layers.Dense(128, activation='relu')\n",
        "])\n",
        "\n",
        "hidden2 = tf.keras.models.Sequential([\n",
        "  hidden1, tf.keras.layers.Dense(128, activation='relu')\n",
        "])\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  hidden2, #tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(10)\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "l2hiez2eIUz8"
      },
      "source": [
        "For each example the model returns a vector of \"[logits](https://developers.google.com/machine-learning/glossary#logits)\" or \"[log-odds](https://developers.google.com/machine-learning/glossary#log-odds)\" scores, one for each class.\n",
        "\n",
        "Let's look at what the model produces for one example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OeOrNdnkEEcR",
        "colab": {}
      },
      "source": [
        "\n",
        "predictions = model(x_train[:1]).numpy()\n",
        "predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tgjhDQGcIniO"
      },
      "source": [
        "The `tf.nn.softmax` function converts these logits to \"probabilities\" for each class.  Think of these as estimates of the probability that the input is a given digit.  These numbers must sum to 1. Note that the numbers can be of the form D.DD...e-N where N is a small integer that indicates the number of places to shift the decimal to the left so that 8.17...e-1 = .871..., for example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zWSRnQ0WI5eq",
        "colab": {}
      },
      "source": [
        "tf.nn.softmax(predictions).numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "he5u_okAYS4a"
      },
      "source": [
        "Note: It is possible to bake this `tf.nn.softmax` in as the activation function for the last layer of the network. While this can make the model output more directly interpretable, this approach is discouraged as it's impossible to\n",
        "provide an exact and numerically stable loss calculation for all models when using a softmax output. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hQyugpgRIyrA"
      },
      "source": [
        "The `losses.SparseCategoricalCrossentropy` loss takes a vector of logits and the index of the `True` label and returns a scalar loss for each example.\n",
        "\n",
        "This loss is equal to the negative log probability of the the true class:\n",
        "It is zero if the model is sure of the correct class.\n",
        "\n",
        "This untrained model gives probabilities close to random (1/10 for each class), so the initial loss should be close to `-tf.log(1/10) ~= 2.3`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RSkzdv8MD0tT",
        "colab": {}
      },
      "source": [
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "loss_fn(y_train[:1], predictions).numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4fx01C-m7dK",
        "colab_type": "text"
      },
      "source": [
        "Now, we are going to compile the model using Keras, and we will tell it what optimizer to use.  We use Stocastic Gradient Descent (SGD), though Adam is more stable and works faster in this case.  See [this blog](https://ruder.io/optimizing-gradient-descent/) for a discussion."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9foNKHzTD2Vo",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='SGD', # or try 'adam' instead\n",
        "              loss=loss_fn,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ix4mEL65on-w"
      },
      "source": [
        "The `Model.fit` method adjusts the model parameters to minimize the loss.  You can specify the number of epochs to train, where an epoch is a sweep through the whole training set, divided into batches presented in random order.  If you train for only 1 epoch, the performance will not be great if you use SGD.  5 epochs with adam is enough for very good performance.  You can re-run this code block to train for more epochs (provided you haven't changed anything above!)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "y7suUbJXVLqP",
        "colab": {}
      },
      "source": [
        "model.fit(x_train, y_train, epochs=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4mDAAPFqVVgn"
      },
      "source": [
        "The `Model.evaluate` method checks the models performance, usually on a \"[Validation-set](https://developers.google.com/machine-learning/glossary#validation-set)\", although the internal name of the set being used is 'test', not 'validation'.  It is a set of 10,000 items not used in training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "F7dTAzgHDUh7",
        "colab": {}
      },
      "source": [
        "model.evaluate(x_test,  y_test, verbose=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "T4JfEh7kvx6m"
      },
      "source": [
        "After 5 epochs with the adam optimizer, the network achieves close to ~98% accuracy on this validation set. We don't do as well with SGD, especially not after just one epoch.  To learn more, read the [TensorFlow tutorials](https://www.tensorflow.org/tutorials/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Aj8NrlzlJqDG"
      },
      "source": [
        "If you want your model to return a probability, you can wrap the trained model, and attach the softmax to it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rYb6DrEH0GMv",
        "colab": {}
      },
      "source": [
        "probability_model = tf.keras.Sequential([\n",
        "  model,\n",
        "  tf.keras.layers.Softmax()\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wvuuwicrqizz",
        "colab_type": "text"
      },
      "source": [
        "Now we are going to look at the network's output probabilities.  We average the probability vector for each of the digits 0 through 9, then plot these\n",
        "in a matrix, which shows that on average, the network thinks that the correct digit is more likely.\n",
        "\n",
        "After just one epoch with SGD, the network is generally assigning most of the propobility to the correct digit, though 3's and 5's and 4's and 9's tend to have some probability mass on each other.  To allow small probability values to be visible in the display, we have taken each probability to the .5 power before displaying.  The largest actual off-diagonal probability is printed above the display, and we have shown the actual probabilities corresponding to the displayed colors on the colorbar.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSnlD0C7WdP7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predmeans = np.zeros((10,10))\n",
        "prv = .5 # scales values for display to make small values visually distinct\n",
        "outputs = probability_model(x_test).numpy()\n",
        "for yv in range (10):\n",
        "  inds = np.where(y_test==yv)\n",
        "  predmeans[yv,:] = np.mean(outputs[inds],axis=0)\n",
        "tpm = np.copy(predmeans)\n",
        "for yv in range(10): tpm[yv,yv] = 0.0\n",
        "print('largest off-diagonal value is',np.amax(tpm))\n",
        "spredmeans = np.power(predmeans,prv) # we apply the scaling here\n",
        "fig, ax = plt.subplots()\n",
        "cax = ax.imshow(spredmeans,cmap='Purples',vmin=0,vmax=1); \n",
        "cbar =fig.colorbar(cax,cmap='Purples',ticks = [0.0,.2,.4,.6,.8,1.0])\n",
        "tks = np.arange(0,10);tt = plt.xticks(tks);tt = plt.yticks((tks));\n",
        "# next line assumes prv == .5\n",
        "cbar.ax.set_yticklabels(['0.00','0.04','0.16','0.36','0.64', '1.00'])\n",
        "tt = ax.set_ylabel('Correct Digit',fontsize = 14)\n",
        "tt = ax.set_xlabel('Predicted Probability',fontsize=14)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Feko0W0A-z1J",
        "colab_type": "text"
      },
      "source": [
        "Next, we compute the representation similarity matrix of the patterns at the first hidden layer. We use Cosine similarity.  The Cosine ranges between +1 and -1, for patterns that are identical to those that are exactly numerically opposite.  Since the hidden layer pattern elements are all non-negative, the cosines all tend to be positive, and indeed, the smallest values are typically about .5, so we display them scaled between 1 and .5."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6zcj1ULFCJ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hidpats = np.zeros((10000,128),dtype=np.float32)\n",
        "hidpats = hidden1(x_test).numpy()\n",
        "hidmeans = np.zeros((10,128))\n",
        "for yv in range (10):\n",
        "  inds = np.where(y_test==yv)\n",
        "  hidmeans[yv,:] = np.mean(hidpats[inds],axis=0)\n",
        "cdm = sp.spatial.distance.pdist(hidmeans,'cosine')\n",
        "rsm = 1 - sp.spatial.distance.squareform(cdm)\n",
        "plt.imshow(rsm,cmap='Purples',vmin=.4,vmax=1)\n",
        "plt.colorbar(cmap='Purples'); \n",
        "tt = plt.xticks(tks);tt = plt.yticks((tks)); \n",
        "tt = plt.xlabel('Digit',fontsize = 14)\n",
        "tt = plt.ylabel('Digit',fontsize=14)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Egc9mAaG_wQB",
        "colab_type": "text"
      },
      "source": [
        "Finally, we do the same for the patterns at the second hidden layer.  You will see that the pattern is similar but the differences are somewhat more exaggerated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pr5SACxHyCfk",
        "colab": {}
      },
      "source": [
        "hidpats = np.zeros((10000,128),dtype=np.float32)\n",
        "hidpats = hidden2(x_test).numpy()  \n",
        "hidmeans = np.zeros((10,128))\n",
        "for yv in range (10):\n",
        "  inds = np.where(y_test==yv)\n",
        "  hidmeans[yv,:] = np.mean(hidpats[inds],axis=0)\n",
        "cdm = sp.spatial.distance.pdist(hidmeans,'cosine')\n",
        "rsm = 1 - sp.spatial.distance.squareform(cdm)\n",
        "# above makes the matrix square and converts cosine distance to\n",
        "# cosine similarity, producing a represenational similarity matrix\n",
        "plt.imshow(rsm,cmap='Purples',vmin=.4,vmax=1)\n",
        "plt.colorbar(cmap='Purples')\n",
        "tt = plt.xticks(tks);tt = plt.yticks((tks)); \n",
        "tt = plt.xlabel('Digit',fontsize = 14)\n",
        "tt = plt.ylabel('Digit',fontsize=14)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6X4MbHJF6m-",
        "colab_type": "text"
      },
      "source": [
        "From here, you can go back and change things or just run more epochs to see how the results change.  \n",
        "\n",
        "Note: Running model.fit for more epochs and then running the subsequent cells manually seems to work fine. If you make changes before the model.fit cell, things will be out of sync. You should restart your run time, then run the first cell of the notebook, then you can chose *Runtime* -> *Run after* to run the rest of the cells."
      ]
    }
  ]
}